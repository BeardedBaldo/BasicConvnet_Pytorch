{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DogBreedClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMj1O48drqMFDz92MrvTbfw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeardedBaldo/BasicConvnet_Pytorch/blob/main/DogBreedClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_GKct3RQfU6"
      },
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "from PIL import Image\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj9LCVbNTPLI",
        "outputId": "3746613b-166f-4f74-b564-d50c8c897029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## download stanford dog dataset\n",
        "url = \"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\"\n",
        "r = requests.get(url, allow_redirects = True) \n",
        "open(\"images.tar\", \"wb\").write(r.content)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "793579520"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4ErQ7MyTjXr"
      },
      "source": [
        "### untaring the dataset\n",
        "tar = tarfile.open(\"images.tar\")\n",
        "tar.extractall(\"./\")\n",
        "tar.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFdLiRdJuEWw"
      },
      "source": [
        "### check for gpus and assign to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s4z3Lb7qaNP"
      },
      "source": [
        "### image loader function and transform object\n",
        "\n",
        "imageSize = (224, 224) if torch.cuda.is_available() else (128, 128)\n",
        "\n",
        "loader = transforms.Compose([transforms.Resize(imageSize), \n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                                                  (0.229, 0.224, 0.225))\n",
        "                             ])\n",
        "\n",
        "def imageLoader(imagePath):\n",
        "  image = Image.open(imagePath)\n",
        "  image = loader(image).unsqueeze(0)\n",
        "  return image.to(device, torch.float)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0P4gWPAtK8c",
        "outputId": "21fdf062-79a8-428a-a48c-a9f52fb7acc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "samplePath = './Images/n02085620-Chihuahua/n02085620_10074.jpg'\n",
        "\n",
        "sampleImage = imageLoader(samplePath)\n",
        "\n",
        "print(type(sampleImage))\n",
        "print(sampleImage.size())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([1, 3, 224, 224])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD8WlN2r-VAM"
      },
      "source": [
        "### variable initialization\n",
        "dataDir = \"./Images\"\n",
        "testSplit = 0.2\n",
        "valSplit = 0.2\n",
        "epochs = 15\n",
        "featureExtract = True\n",
        "modelName = \"vgg11\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HfNfNXZUklD",
        "outputId": "08246f84-0e21-42b0-802b-bd020b94f899",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### load data and split into train and test\n",
        "\n",
        "## delete 2933 from Shetland sheeps since its a png\n",
        "\n",
        "dataDict = {\"train\": {\n",
        "    \"images\": [],\n",
        "    \"labels\": []\n",
        "}, \"validation\": {\n",
        "    \"images\": [],\n",
        "    \"labels\": []    \n",
        "}}\n",
        "trainLabels = []\n",
        "trainImages = []\n",
        "mapping = []\n",
        "testLabels = []\n",
        "testImages = []\n",
        "\n",
        "sampleList = ['./Images/n02108000-EntleBucher', './Images/n02111889-Samoyed']\n",
        "\n",
        "for i, (root, dirs, files) in enumerate(os.walk(dataDir)):\n",
        "  if root != dataDir and i<80:# and root in sampleList:    ### add root in sampleList condition only for testing\n",
        "    name = root.split(\"/\")[-1].split(\"-\")[-1]\n",
        "    mapping.append(name)\n",
        "    print(root)\n",
        "    print(name, \" : \", i)\n",
        "    nFiles = len(files)\n",
        "    for j, f in enumerate(files):\n",
        "      filePath = os.path.join(root, f)\n",
        "      image = imageLoader(filePath)\n",
        "      if j < (nFiles * (1 - testSplit)):\n",
        "        trainImages.append(image)\n",
        "        trainLabels.append(i-1)\n",
        "      else:\n",
        "        testImages.append(image)\n",
        "        testLabels.append(i-1)\n",
        "      del image\n",
        "\n",
        "trainData = {}\n",
        "trainData[\"trainImages\"] = trainImages\n",
        "trainData[\"trainLabels\"] = trainLabels\n",
        "nClasses = len(mapping)\n",
        "    \n",
        "print(trainData.keys())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./Images/n02108422-bull_mastiff\n",
            "bull_mastiff  :  1\n",
            "./Images/n02113978-Mexican_hairless\n",
            "Mexican_hairless  :  2\n",
            "./Images/n02087046-toy_terrier\n",
            "toy_terrier  :  3\n",
            "./Images/n02085936-Maltese_dog\n",
            "Maltese_dog  :  4\n",
            "./Images/n02098286-West_Highland_white_terrier\n",
            "West_Highland_white_terrier  :  5\n",
            "./Images/n02089973-English_foxhound\n",
            "English_foxhound  :  6\n",
            "./Images/n02111889-Samoyed\n",
            "Samoyed  :  7\n",
            "./Images/n02097047-miniature_schnauzer\n",
            "miniature_schnauzer  :  8\n",
            "./Images/n02093991-Irish_terrier\n",
            "Irish_terrier  :  9\n",
            "./Images/n02094433-Yorkshire_terrier\n",
            "Yorkshire_terrier  :  10\n",
            "./Images/n02097209-standard_schnauzer\n",
            "standard_schnauzer  :  11\n",
            "./Images/n02098413-Lhasa\n",
            "Lhasa  :  12\n",
            "./Images/n02096585-Boston_bull\n",
            "Boston_bull  :  13\n",
            "./Images/n02113624-toy_poodle\n",
            "toy_poodle  :  14\n",
            "./Images/n02100236-German_short-haired_pointer\n",
            "haired_pointer  :  15\n",
            "./Images/n02105251-briard\n",
            "briard  :  16\n",
            "./Images/n02110958-pug\n",
            "pug  :  17\n",
            "./Images/n02113712-miniature_poodle\n",
            "miniature_poodle  :  18\n",
            "./Images/n02094114-Norfolk_terrier\n",
            "Norfolk_terrier  :  19\n",
            "./Images/n02108089-boxer\n",
            "boxer  :  20\n",
            "./Images/n02107908-Appenzeller\n",
            "Appenzeller  :  21\n",
            "./Images/n02091467-Norwegian_elkhound\n",
            "Norwegian_elkhound  :  22\n",
            "./Images/n02107683-Bernese_mountain_dog\n",
            "Bernese_mountain_dog  :  23\n",
            "./Images/n02097298-Scotch_terrier\n",
            "Scotch_terrier  :  24\n",
            "./Images/n02086240-Shih-Tzu\n",
            "Tzu  :  25\n",
            "./Images/n02105162-malinois\n",
            "malinois  :  26\n",
            "./Images/n02112706-Brabancon_griffon\n",
            "Brabancon_griffon  :  27\n",
            "./Images/n02099712-Labrador_retriever\n",
            "Labrador_retriever  :  28\n",
            "./Images/n02106382-Bouvier_des_Flandres\n",
            "Bouvier_des_Flandres  :  29\n",
            "./Images/n02101006-Gordon_setter\n",
            "Gordon_setter  :  30\n",
            "./Images/n02110806-basenji\n",
            "basenji  :  31\n",
            "./Images/n02088632-bluetick\n",
            "bluetick  :  32\n",
            "./Images/n02102318-cocker_spaniel\n",
            "cocker_spaniel  :  33\n",
            "./Images/n02089867-Walker_hound\n",
            "Walker_hound  :  34\n",
            "./Images/n02088466-bloodhound\n",
            "bloodhound  :  35\n",
            "./Images/n02104029-kuvasz\n",
            "kuvasz  :  36\n",
            "./Images/n02096437-Dandie_Dinmont\n",
            "Dandie_Dinmont  :  37\n",
            "./Images/n02086079-Pekinese\n",
            "Pekinese  :  38\n",
            "./Images/n02089078-black-and-tan_coonhound\n",
            "tan_coonhound  :  39\n",
            "./Images/n02096294-Australian_terrier\n",
            "Australian_terrier  :  40\n",
            "./Images/n02102480-Sussex_spaniel\n",
            "Sussex_spaniel  :  41\n",
            "./Images/n02093428-American_Staffordshire_terrier\n",
            "American_Staffordshire_terrier  :  42\n",
            "./Images/n02100877-Irish_setter\n",
            "Irish_setter  :  43\n",
            "./Images/n02102177-Welsh_springer_spaniel\n",
            "Welsh_springer_spaniel  :  44\n",
            "./Images/n02111129-Leonberg\n",
            "Leonberg  :  45\n",
            "./Images/n02106166-Border_collie\n",
            "Border_collie  :  46\n",
            "./Images/n02088094-Afghan_hound\n",
            "Afghan_hound  :  47\n",
            "./Images/n02107142-Doberman\n",
            "Doberman  :  48\n",
            "./Images/n02102040-English_springer\n",
            "English_springer  :  49\n",
            "./Images/n02109047-Great_Dane\n",
            "Great_Dane  :  50\n",
            "./Images/n02085782-Japanese_spaniel\n",
            "Japanese_spaniel  :  51\n",
            "./Images/n02105641-Old_English_sheepdog\n",
            "Old_English_sheepdog  :  52\n",
            "./Images/n02115641-dingo\n",
            "dingo  :  53\n",
            "./Images/n02100735-English_setter\n",
            "English_setter  :  54\n",
            "./Images/n02101388-Brittany_spaniel\n",
            "Brittany_spaniel  :  55\n",
            "./Images/n02113186-Cardigan\n",
            "Cardigan  :  56\n",
            "./Images/n02091032-Italian_greyhound\n",
            "Italian_greyhound  :  57\n",
            "./Images/n02091635-otterhound\n",
            "otterhound  :  58\n",
            "./Images/n02109525-Saint_Bernard\n",
            "Saint_Bernard  :  59\n",
            "./Images/n02097474-Tibetan_terrier\n",
            "Tibetan_terrier  :  60\n",
            "./Images/n02105056-groenendael\n",
            "groenendael  :  61\n",
            "./Images/n02112137-chow\n",
            "chow  :  62\n",
            "./Images/n02106550-Rottweiler\n",
            "Rottweiler  :  63\n",
            "./Images/n02099601-golden_retriever\n",
            "golden_retriever  :  64\n",
            "./Images/n02115913-dhole\n",
            "dhole  :  65\n",
            "./Images/n02107574-Greater_Swiss_Mountain_dog\n",
            "Greater_Swiss_Mountain_dog  :  66\n",
            "./Images/n02090379-redbone\n",
            "redbone  :  67\n",
            "./Images/n02086646-Blenheim_spaniel\n",
            "Blenheim_spaniel  :  68\n",
            "./Images/n02106662-German_shepherd\n",
            "German_shepherd  :  69\n",
            "./Images/n02097658-silky_terrier\n",
            "silky_terrier  :  70\n",
            "./Images/n02108551-Tibetan_mastiff\n",
            "Tibetan_mastiff  :  71\n",
            "./Images/n02106030-collie\n",
            "collie  :  72\n",
            "./Images/n02099429-curly-coated_retriever\n",
            "coated_retriever  :  73\n",
            "./Images/n02100583-vizsla\n",
            "vizsla  :  74\n",
            "./Images/n02110185-Siberian_husky\n",
            "Siberian_husky  :  75\n",
            "./Images/n02105412-kelpie\n",
            "kelpie  :  76\n",
            "./Images/n02104365-schipperke\n",
            "schipperke  :  77\n",
            "./Images/n02110627-affenpinscher\n",
            "affenpinscher  :  78\n",
            "./Images/n02091134-whippet\n",
            "whippet  :  79\n",
            "dict_keys(['trainImages', 'trainLabels'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHQjpP8-izmY"
      },
      "source": [
        "### train validation split \n",
        "trainImages, validationImages, trainLabels, validationLabels = train_test_split(trainImages, trainLabels, test_size = valSplit)\n",
        "dataDict[\"train\"][\"images\"] = trainImages\n",
        "dataDict[\"validation\"][\"images\"] = validationImages\n",
        "dataDict[\"train\"][\"labels\"] = trainLabels\n",
        "dataDict[\"validation\"][\"labels\"] = validationLabels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9XqHFQN_ZK0"
      },
      "source": [
        "### set requires_grad for model parameters\n",
        "\n",
        "def setParameterRequiresGrad(model, featureExract):\n",
        "  if featureExtract:\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8XyMzsLcaHZ",
        "outputId": "d570261d-f1cf-40a0-f51a-f2a376932d8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### model initialization\n",
        "\n",
        "def initializeModel(modelName, nClasses, featureExtract, \n",
        "                    use_pretrained = True):\n",
        "  modelFt = None\n",
        "  inputSize = 0\n",
        "\n",
        "  if modelName == \"resnet\":\n",
        "    modelFt = models.resnet50(pretrained = use_pretrained)\n",
        "    setParameterRequiresGrad(modelFt, featureExtract)\n",
        "    nFeatures = modelFt.fc.in_features\n",
        "    modelFt.fc = nn.Linear(nFeatures, nClasses)\n",
        "    inputSize = 224\n",
        "\n",
        "  elif modelName == \"vgg11\":\n",
        "    modelFt = models.vgg11_bn(pretrained=use_pretrained)\n",
        "    setParameterRequiresGrad(modelFt, featureExtract)\n",
        "    nFeatures = modelFt.classifier[6].in_features\n",
        "    modelFt.classifier[6] = nn.Linear(nFeatures, nClasses)\n",
        "    inputSize = 224\n",
        "\n",
        "  elif modelName == \"inception\":\n",
        "    modelFt = models.inception_v3(pretrained = use_pretrained)\n",
        "    setParameterRequiresGrad(modelFt, featureExtract)\n",
        "    nFeatures = modelFt.AuxLogits.fc.in_features\n",
        "    modelFt.AuxLogits.fc = nn.Linear(nFeatures, nClasses)\n",
        "    nFeatures = modelFt.fc.in_features\n",
        "    modelFt.fc = nn.Linear(nFeatures, nClasses)\n",
        "    inputSize = 299\n",
        "\n",
        "  else:\n",
        "    print(\"Invalid model name, exiting...\")\n",
        "    exit()\n",
        "\n",
        "  return modelFt, inputSize \n",
        "\n",
        "modelFt, inputSize = initializeModel(modelName, nClasses,\n",
        "                                     featureExtract)\n",
        "print(modelFt)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=79, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIcA9SuFeoro",
        "outputId": "f167661d-a24f-46ff-b715-ebdfdc9fcf68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### model GPU initialization, optimizer initialization\n",
        "\n",
        "## send model to GPU\n",
        "modelFt = modelFt.to(device)\n",
        "\n",
        "\n",
        "## print parameters to learn\n",
        "print(\"Parameters to learn\")\n",
        "if featureExtract:\n",
        "  paramsUpdate = []\n",
        "  for name, param in modelFt.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "      paramsUpdate.append(param)\n",
        "      print(\"\\t\", name)\n",
        "else:\n",
        "  paramsUpdate = modelFt.parameters()\n",
        "  for name, param in modelFt.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "      print(\"\\t\", name)\n",
        "\n",
        "\n",
        "## initialize optimizer\n",
        "optimizerFt = optim.Adam(paramsUpdate, lr = 0.001) "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters to learn\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj_8-zkNlF0g"
      },
      "source": [
        "### model training function\n",
        "\n",
        "def trainModel(model, trainData, criterion, optimizer,\n",
        "               nEpochs = 10, isInception = False):\n",
        "  start = time.time()\n",
        "\n",
        "  trainAccHistory = []\n",
        "  valAccHistory = []\n",
        "\n",
        "  bestModelWts = copy.deepcopy(model.state_dict())\n",
        "  bestAcc = 0\n",
        "\n",
        "  for epoch in range(nEpochs):\n",
        "    print(f'Epoch {epoch}/{nEpochs - 1}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    for phase in dataDict.keys():\n",
        "      if phase == \"train\":\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      runningLoss = 0.0\n",
        "      runningCorrects = 0\n",
        "\n",
        "      inputs = dataDict[phase][\"images\"]\n",
        "      labels = dataDict[phase][\"labels\"]\n",
        "\n",
        "      for i, input in enumerate(inputs):\n",
        "        input = input.to(device)\n",
        "        label = []\n",
        "        label.append(labels[i])\n",
        "        label = torch.tensor(np.array(label)).to(device)\n",
        "        \n",
        "        ##zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(phase == \"train\"):\n",
        "\n",
        "          if isInception:\n",
        "            output, auxOutput = model(input)\n",
        "            mainLoss = criterion(output, label)\n",
        "            auxLoss = criterion(auxOutput, label)\n",
        "            loss = mainLoss + 0.4 * auxLoss\n",
        "          else:\n",
        "            output = model(input)\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "          _, prediction = torch.max(output, 1)\n",
        "\n",
        "          if phase == \"train\":\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        runningLoss += loss.item() * input.size(0)\n",
        "        runningCorrects += torch.sum(prediction == label)\n",
        "\n",
        "      epochLoss = runningLoss / (len(inputs))\n",
        "      epochAcc = runningCorrects.double() / (len(inputs))\n",
        "      \n",
        "      print(\"{} loss: {:4f}, {} accuracy: {:4f}\".format(phase,\n",
        "                                                        epochLoss,\n",
        "                                                        phase,\n",
        "                                                        epochAcc))\n",
        "      if phase == \"train\":\n",
        "        trainAccHistory.append(epochAcc)\n",
        "      else:\n",
        "        valAccHistory.append(epochAcc)\n",
        "\n",
        "      if phase == \"validation\" and epochAcc > bestAcc:\n",
        "        bestAcc = epochAcc\n",
        "        bestModelWts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "\n",
        "      \n",
        "\n",
        "  timeTaken = time.time() - start\n",
        "  print(\"training complete in {:.0f}m {:.0f}s\".format(timeTaken // 60,\n",
        "                                         timeTaken % 60))\n",
        "  \n",
        "  print(\"Best validation accuracy: {:4f}\".format(bestAcc))\n",
        "\n",
        "  model.load_state_dict(bestModelWts)\n",
        "\n",
        "  return model, trainAccHistory, valAccHistory\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z75oym7A74n",
        "outputId": "b300461b-cf6c-4447-81be-d180362ea804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "modelFt, trainhist, valHist = trainModel(modelFt, trainData, criterion,\n",
        "                           optimizerFt, nEpochs = epochs,\n",
        "                           isInception = (modelName == \"inception\"))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/14\n",
            "----------\n",
            "train loss: 4.534700, train accuracy: 0.170806\n",
            "validation loss: 2.034903, validation accuracy: 0.653864\n",
            "Epoch 1/14\n",
            "----------\n",
            "train loss: 4.244231, train accuracy: 0.277648\n",
            "validation loss: 2.453505, validation accuracy: 0.666042\n",
            "Epoch 2/14\n",
            "----------\n",
            "train loss: 4.311552, train accuracy: 0.313964\n",
            "validation loss: 2.696318, validation accuracy: 0.675878\n",
            "Epoch 3/14\n",
            "----------\n",
            "train loss: 4.412449, train accuracy: 0.336692\n",
            "validation loss: 2.829746, validation accuracy: 0.688993\n",
            "Epoch 4/14\n",
            "----------\n",
            "train loss: 4.453139, train accuracy: 0.349930\n",
            "validation loss: 3.262480, validation accuracy: 0.678220\n",
            "Epoch 5/14\n",
            "----------\n",
            "train loss: 4.646470, train accuracy: 0.360005\n",
            "validation loss: 3.353479, validation accuracy: 0.671194\n",
            "Epoch 6/14\n",
            "----------\n",
            "train loss: 4.624108, train accuracy: 0.374297\n",
            "validation loss: 3.533420, validation accuracy: 0.678220\n",
            "Epoch 7/14\n",
            "----------\n",
            "train loss: 4.706920, train accuracy: 0.385309\n",
            "validation loss: 3.727799, validation accuracy: 0.680562\n",
            "Epoch 8/14\n",
            "----------\n",
            "train loss: 4.728217, train accuracy: 0.383669\n",
            "validation loss: 3.412237, validation accuracy: 0.701639\n",
            "Epoch 9/14\n",
            "----------\n",
            "train loss: 4.698238, train accuracy: 0.397610\n",
            "validation loss: 3.804730, validation accuracy: 0.690867\n",
            "Epoch 10/14\n",
            "----------\n",
            "train loss: 4.903597, train accuracy: 0.389058\n",
            "validation loss: 4.043499, validation accuracy: 0.687119\n",
            "Epoch 11/14\n",
            "----------\n",
            "train loss: 4.879042, train accuracy: 0.399250\n",
            "validation loss: 4.253483, validation accuracy: 0.678220\n",
            "Epoch 12/14\n",
            "----------\n",
            "train loss: 4.937258, train accuracy: 0.409091\n",
            "validation loss: 4.271898, validation accuracy: 0.677752\n",
            "Epoch 13/14\n",
            "----------\n",
            "train loss: 4.810304, train accuracy: 0.423032\n",
            "validation loss: 4.341027, validation accuracy: 0.686183\n",
            "Epoch 14/14\n",
            "----------\n",
            "train loss: 5.036590, train accuracy: 0.413543\n",
            "validation loss: 4.272021, validation accuracy: 0.693208\n",
            "training complete in 18m 14s\n",
            "Best validation accuracy: 0.701639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byfd6O6JvpeE",
        "outputId": "5a88d23b-fd50-4765-de0a-dfd536e61078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "###\n",
        "tHist = [h.cpu().numpy() for h in trainhist]\n",
        "vHist = [h.cpu().numpy() for h in valHist]\n",
        "\n",
        "plt.xlabel(\"training epochs\")\n",
        "plt.ylabel(\"training accuracy\")\n",
        "plt.plot(range(1, epochs+1), tHist)\n",
        "plt.plot(range(1, epochs+1), vHist)\n",
        "plt.ylim((0, 1.))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tCfmE94nWW7",
        "outputId": "a04a9a6b-a6fa-4919-d0ef-67648172ef93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### model evaluation\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  modelFt.eval()\n",
        "  inputs = testImages\n",
        "  labels = testLabels\n",
        "  for i, input in enumerate(inputs):\n",
        "      input = input.to(device)\n",
        "      label = []\n",
        "      label.append(labels[i])\n",
        "      label = torch.tensor(np.array(label)).to(device)\n",
        "      output = modelFt(input)\n",
        "      _, predicted = torch.max(output, 1)\n",
        "      #print(\"Label:\", int(label[0]), \", Predicted:\", int(predicted[0]))\n",
        "      total += label.size(0)\n",
        "      correct += (predicted==label).sum().item()\n",
        "\n",
        "print(\"Testing accuracy: \", (correct/total) * 100)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy:  69.56686930091185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwzKcoRGzjEp"
      },
      "source": [
        "### prediction \n",
        "predictPath = \"./Images/n02086240-Shih-Tzu/n02086240_1011.jpg\"\n",
        "\n",
        "def predictBreed(predictPath, model, mapping):\n",
        "  input = imageLoader(predictPath)\n",
        "  with torch.no_grad():\n",
        "    input = input.to(device)\n",
        "    output = model(input)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    predictedLabel = int(predicted[0])\n",
        "    predictedBreed = mapping[predictedLabel]\n",
        "\n",
        "  return predictedBreed\n",
        "\n",
        "predictBreed = predictBreed(predictPath, modelFt, mapping)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_7kh-MAkY3y",
        "outputId": "b7a1c925-adb6-4331-e769-cec010acb08d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(predictBreed)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tzu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTmqfOTxkB7Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}